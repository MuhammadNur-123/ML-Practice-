{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a05e9d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\IT\n",
      "[nltk_data]     BD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cb731f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\it bd\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\it bd\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\it bd\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\it bd\\appdata\\roaming\\python\\python311\\site-packages (from nltk) (2023.6.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\it bd\\appdata\\roaming\\python\\python311\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\it bd\\appdata\\roaming\\python\\python311\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3f204f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "word = ['change', 'changing', 'changes','changed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ee0257b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['change', 'changing', 'changes', 'changed']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15b2fd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb95b2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1312fe7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chang\n",
      "chang\n",
      "chang\n",
      "chang\n"
     ]
    }
   ],
   "source": [
    "for w in word:\n",
    "    print(p.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a9073da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "change chang\n",
      "changing chang\n",
      "changes chang\n",
      "changed chang\n"
     ]
    }
   ],
   "source": [
    "for w in word:\n",
    "    print(w,p.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb4cc30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sen = 'I want to change the world if world changed my career by changing abcd'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8874d49b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I want to change the world if world changed my career by changing abcd'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3410ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7003368",
   "metadata": {},
   "outputs": [],
   "source": [
    "toke = word_tokenize(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f716d3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'want',\n",
       " 'to',\n",
       " 'change',\n",
       " 'the',\n",
       " 'world',\n",
       " 'if',\n",
       " 'world',\n",
       " 'changed',\n",
       " 'my',\n",
       " 'career',\n",
       " 'by',\n",
       " 'changing',\n",
       " 'abcd']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f649c0e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'want',\n",
       " 'to',\n",
       " 'change',\n",
       " 'the',\n",
       " 'world',\n",
       " 'if',\n",
       " 'world',\n",
       " 'changed',\n",
       " 'my',\n",
       " 'career',\n",
       " 'by',\n",
       " 'changing',\n",
       " 'abcd']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sen.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3bb8c5e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I i\n",
      "want want\n",
      "to to\n",
      "change chang\n",
      "the the\n",
      "world world\n",
      "if if\n",
      "world world\n",
      "changed chang\n",
      "my my\n",
      "career career\n",
      "by by\n",
      "changing chang\n",
      "abcd abcd\n"
     ]
    }
   ],
   "source": [
    "for w in toke:\n",
    "    print(w , p.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97af665c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\IT\n",
      "[nltk_data]     BD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "25e4bd52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\IT\n",
      "[nltk_data]     BD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1b0084",
   "metadata": {},
   "source": [
    "# Lemmatization in NPL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "69d2fb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "96c975ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "le =  WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d377c6a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'want',\n",
       " 'to',\n",
       " 'change',\n",
       " 'the',\n",
       " 'world',\n",
       " 'if',\n",
       " 'world',\n",
       " 'changed',\n",
       " 'my',\n",
       " 'career',\n",
       " 'by',\n",
       " 'changing',\n",
       " 'abcd']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0c01078c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I I\n",
      "want want\n",
      "to to\n",
      "change change\n",
      "the the\n",
      "world world\n",
      "if if\n",
      "world world\n",
      "changed changed\n",
      "my my\n",
      "career career\n",
      "by by\n",
      "changing changing\n",
      "abcd abcd\n"
     ]
    }
   ],
   "source": [
    "for w in toke:\n",
    "    print(w , le.lemmatize(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "720d135a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'change'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.lemmatize('changes')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960fc73b",
   "metadata": {},
   "source": [
    "# Tokenization in NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637dfae7",
   "metadata": {},
   "source": [
    "In Python, there are several libraries and tools available for performing tokenization and other NLP tasks. Here are a few examples using popular libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefba15a",
   "metadata": {},
   "source": [
    "# NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ff41c9",
   "metadata": {},
   "source": [
    "NLTK (Natural Language Toolkit) is a widely used library for NLP tasks. To perform tokenization using NLTK, you need to install it first. You can do so by running pip install nltk. Here's an example of tokenizing a sentence using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "81dda374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'from', 'aiquest', 'Intelligence', '.', 'I', 'am', 'Larning', 'NPL', '.', 'It', 'is', 'fasinathing', '!']\n",
      "['I am from aiquest Intelligence.', 'I am Larning NPL.', 'It is fasinathing!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "sentence =\"I am from aiquest Intelligence. I am Larning NPL. It is fasinathing!\"\n",
    "word_tokens =word_tokenize(sentence)\n",
    "sentence_tokens =sent_tokenize(sentence)\n",
    "print(word_tokens)\n",
    "print(sentence_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510b02e2",
   "metadata": {},
   "source": [
    "# spaCy  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b9fb669b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', \"'m\", 'from', 'aiQuest', 'Intelligence', '.', 'I', 'am', 'learning', 'NLP', '.', 'It', 'is', 'fascinating', '!']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')  # Load the English language model\n",
    "\n",
    "sentence = \"I'm from aiQuest Intelligence. I am learning NLP. It is fascinating!\"\n",
    "doc = nlp(sentence)\n",
    "\n",
    "word_tokens = [token.text for token in doc]\n",
    "\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a685e3ec",
   "metadata": {},
   "source": [
    "# Transformers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cc6698ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neither PyTorch nor TensorFlow >= 2.0 have been found.Models won't be available and only tokenizers, configurationand file/data utilities can be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', \"'\", 'm', 'from', 'ai', '##quest', 'intelligence', '.', 'i', 'am', 'learning', 'nl', '##p', '.', 'it', 'is', 'fascinating', '!']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "sentence = \"I'm from aiQuest Intelligence. I am learning NLP. It is fascinating!\"\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d9be5b",
   "metadata": {},
   "source": [
    "# Named Entity Tokenization using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "877f7513",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\IT BD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8e65ee88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\IT\n",
      "[nltk_data]     BD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3a8db0aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to C:\\Users\\IT\n",
      "[nltk_data]     BD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data] Downloading package words to C:\\Users\\IT\n",
      "[nltk_data]     BD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2451eae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading maxent_ne_chuker: Package 'maxent_ne_chuker'\n",
      "[nltk_data]     not found in index\n",
      "[nltk_data] Downloading package words to C:\\Users\\IT\n",
      "[nltk_data]     BD\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aiQuestIntelligence', 'NLP', 'Nur', 'Muhammad']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping corpora\\words.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('maxent_ne_chuker')\n",
    "nltk.download('words')\n",
    "from nltk import word_tokenize, pos_tag,ne_chunk\n",
    "sentence = \"I'm from aiQuest Intelligence. I am learning NLP. It is fascinating!, my name is Nur ,Muhammad ,jahid bangladesh\"\n",
    "tokens = word_tokenize(sentence)\n",
    "\n",
    "pos_tags = pos_tag(tokens)\n",
    "ner_tags = ne_chunk(pos_tags)\n",
    "named_entity_tokens = []\n",
    "for chunk in ner_tags:\n",
    "    if hasattr(chunk, 'label'):\n",
    "        named_entity_tokens.append(''.join(c[0] for c in chunk))\n",
    "print(named_entity_tokens)        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78125e8",
   "metadata": {},
   "source": [
    "# Text Vactorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "61270099",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I love Bangladesh</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Could you give me an iphone?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hello how are you?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I want to talk you.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           test  class\n",
       "0             I love Bangladesh      1\n",
       "1  Could you give me an iphone?      0\n",
       "2            Hello how are you?      1\n",
       "3           I want to talk you.      1"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('data.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d158ab",
   "metadata": {},
   "source": [
    "# CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "350463b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a778e2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d5864bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_z = tf.fit_transform(df['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8756e684",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x14 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 16 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4d8a7084",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TfidfVectorizer' object has no attribute 'get_feature_names'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m cv_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(tf_z\u001b[38;5;241m.\u001b[39mtoarray(), columns\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mget_feature_names(), index\u001b[38;5;241m=\u001b[39mdf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'TfidfVectorizer' object has no attribute 'get_feature_names'"
     ]
    }
   ],
   "source": [
    "cv_df = pd.DataFrame(tf_z.toarray(), columns=tf.get_feature_names(), index=df['test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23691446",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9dfd35b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\it bd\\anaconda3\\lib\\site-packages (4.3.0)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\it bd\\anaconda3\\lib\\site-packages (from gensim) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\it bd\\anaconda3\\lib\\site-packages (from gensim) (1.11.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\it bd\\anaconda3\\lib\\site-packages (from gensim) (5.2.1)\n",
      "Requirement already satisfied: FuzzyTM>=0.4.0 in c:\\users\\it bd\\anaconda3\\lib\\site-packages (from gensim) (2.0.5)\n",
      "Requirement already satisfied: pandas in c:\\users\\it bd\\anaconda3\\lib\\site-packages (from FuzzyTM>=0.4.0->gensim) (2.0.3)\n",
      "Requirement already satisfied: pyfume in c:\\users\\it bd\\anaconda3\\lib\\site-packages (from FuzzyTM>=0.4.0->gensim) (0.2.25)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\it bd\\anaconda3\\lib\\site-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\it bd\\anaconda3\\lib\\site-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2022.7)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\it bd\\anaconda3\\lib\\site-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2023.3)\n",
      "Requirement already satisfied: simpful in c:\\users\\it bd\\anaconda3\\lib\\site-packages (from pyfume->FuzzyTM>=0.4.0->gensim) (2.11.1)\n",
      "Requirement already satisfied: fst-pso in c:\\users\\it bd\\anaconda3\\lib\\site-packages (from pyfume->FuzzyTM>=0.4.0->gensim) (1.8.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\it bd\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.8.2->pandas->FuzzyTM>=0.4.0->gensim) (1.16.0)\n",
      "Requirement already satisfied: miniful in c:\\users\\it bd\\anaconda3\\lib\\site-packages (from fst-pso->pyfume->FuzzyTM>=0.4.0->gensim) (0.0.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2ae8d75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d5cd0019",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['I', 'love', 'Bangladesh'],\n",
       " ['Could', 'you', 'give', 'me', 'an', 'iphone', '?'],\n",
       " ['Hello', 'how', 'are', 'you', '?'],\n",
       " ['I', 'want', 'to', 'talk', 'you', '.']]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vector = [nltk.word_tokenize(test) for test in df['test']]\n",
    "text_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d345d1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(text_vector, min_count=1) #shift+tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ee9f2a06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('an', 0.17826786637306213),\n",
       " ('I', 0.16072483360767365),\n",
       " ('give', 0.10560770332813263),\n",
       " ('how', 0.09215974807739258),\n",
       " ('iphone', 0.048910051584243774),\n",
       " ('are', 0.02700837142765522),\n",
       " ('Could', 0.007729300297796726),\n",
       " ('you', -0.03771638125181198),\n",
       " ('.', -0.04552280902862549),\n",
       " ('talk', -0.0464920699596405)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('want')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e11824",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
